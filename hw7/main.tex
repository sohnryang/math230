\documentclass{scrartcl}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{blindtext}
\usepackage{datetime}
\usepackage{fontspec}
\usepackage{graphicx}
\usepackage{kotex}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{pgf,tikz,pgfplots}
\usepackage{float}

\pgfplotsset{compat=1.15}
\usetikzlibrary{arrows}

\newcommand\Overline[2][0.8pt]{%
  \begin{tikzpicture}[baseline=(a.base)]
    \node[inner xsep=0pt,inner ysep=1.5pt] (a) {$#2$};
    \draw[line width= #1] (a.north west) -- (a.north east);
  \end{tikzpicture}
}
\newtheorem{theorem}{Theorem}

\setmainhangulfont{Noto Serif CJK KR}[
  UprightFont=* Light, BoldFont=* Bold,
  Script=Hangul, Language=Korean, AutoFakeSlant,
]
\setsanshangulfont{Noto Sans CJK KR}[
  UprightFont=* DemiLight, BoldFont=* Medium,
  Script=Hangul, Language=Korean
]
\setmathhangulfont{Noto Sans CJK KR}[
  SizeFeatures={
    {Size=-6,  Font=* Medium},
    {Size=6-9, Font=*},
    {Size=9-,  Font=* DemiLight},
  },
  Script=Hangul, Language=Korean
]
\title{MATH230: Homework 7 (due Oct. 30)}
\author{손량(20220323)}
\date{Last compiled on: \today, \currenttime}

\newcommand{\un}[1]{\ensuremath{\ \mathrm{#1}}}
\newcommand{\imag}{\operatorname{Im}}
\newcommand{\real}{\operatorname{Re}}
\newcommand{\Log}{\operatorname{Log}}
\newcommand{\Arg}{\operatorname{Arg}}
\DeclareMathOperator*{\Res}{Res}

\begin{document}
\maketitle

\section{Chapter 7 \#4}
Let \(Z_1 = X + Y\), \(Z_2 = Y\) and \(g(z_1, z_2) = P(Z_1 = z_1, Z_2 = z_2)\).
Then we can write
\begin{align*}
  g(z_1, z_2)
  = f(z_1 - z_2, z_2)
  = \begin{cases}
    \frac{z_1 + z_2}{27} & (z_2 \in \{0, 1, 2\}, z_1 - z_2 \in \{0, 1, 2\}) \\
    0 & (\text{otherwise})
  \end{cases}
\end{align*}
In conclusion, we can calculate the marginal distribution of \(Z_1 = Z\).
\begin{align*}
  P(Z = 0)
  &= g(0, 0)
  = 0 \\
  P(Z = 1)
  &= g(1, 0) + g(1, 1)
  = \frac{1}{9} \\
  P(Z = 2)
  &= g(2, 0) + g(2, 1) + g(2, 2)
  = \frac{1}{3} \\
  P(Z = 3)
  &= g(3, 1) + g(3, 2)
  = \frac{1}{3} \\
  P(Z = 4)
  &= g(4, 2)
  = \frac{2}{9}
\end{align*}

\section{Chapter 7 \#5}
Let \(w(y) = \exp(-y / 2)\). Then \(w(Y) = X\) holds, so we can write
\begin{align*}
  g(y)
  = f(w(y)) |w'(y)|
  = \begin{cases}
    \frac{\exp(-y / 2)}{2} & (y > 0) \\
    0 & (y \le 0)
  \end{cases}
\end{align*}
and \(g(y)\) is probability density function of \(Y\). Thus, \(Y\) follows an
exponential distribution with \(\lambda = 1/2\).

\section{Chapter 7 \#7}
Let \(\phi(w) = \sqrt{2w / m}\). Then \(\phi(W) = V\) holds, so we can write
\begin{align*}
  g(w)
  = f(\phi(w)) |\phi'(w)|
  = \begin{cases}
    \frac{k \sqrt{2w}}{m^{3/2}} \exp(-2bw / m) & (w > 0) \\
    0 & (w \le 0)
  \end{cases}
\end{align*}
which is a probability distribution of \(W\).

\section{Chapter 7 \#12}
Let \(w_1(y_1, y_2) = y_1 y_2, w_2(y_1, y_2) = y_1 - y_1 y_2\). Then \(X_1 =
w_1(Y_1, Y_2), X_2 = w_2(Y_1, Y_2)\), so
\begin{align*}
  J
  = \det \begin{pmatrix}
    \frac{\partial w_1}{\partial y_1} & \frac{\partial w_1}{\partial y_2} \\
    \frac{\partial w_2}{\partial y_1} & \frac{\partial w_2}{\partial y_2}
  \end{pmatrix}
  = \det \begin{pmatrix}
    y_2 & y_1 \\
    1 - y_2 & -y_1
  \end{pmatrix}
  = -y_1
\end{align*}
Then, we can write the joint probability density of \(Y_1\) and \(Y_2\) as
\begin{align*}
  g(y_1, y_2)
  &= f(w_1(y_1, y_2)) f(w_2(y_1, y_2)) |J| \\
  &= \begin{cases}
    e^{-y_1 y_2} e^{-y_1 + y_1 y_2} |y_1| & (y_1 y_2 > 0, y_1 - y_1 y_2 > 0) \\
    0 & (\text{elsewhere})
  \end{cases} \\
  &= \begin{cases}
    e^{-y_1} |y_1| & (y_1 y_2 > 0, y_1 - y_1 y_2 > 0) \\
    0 & (\text{elsewhere})
  \end{cases}
\end{align*}
The marginal distribution of \(Y_1\) can be obtained as
\begin{align*}
  h_1(y_1)
  = \int^\infty_{-\infty} g(y_1, y_2) dy_2
  = \int^1_0 e^{-y_1} |y_1| dy_2
  = e^{-y_1} |y_1|
\end{align*}
for \(y_1 > 0\). For \(y_1 \le 0\), \(g(y_1, y_2)\) is zero so \(h_1(y_1) =
0\). Similarly, the marginal distribution of \(Y_2\) can be obtained as
\begin{align*}
  h_2(y_2)
  &= \int^\infty_{-\infty} g(y_1, y_2) dy_1
  = \int^\infty_0 e^{-y_1} |y_1| dy_1
  = \int^\infty_0 y_1 e^{-y_1} dy_1 \\
  &= \left[ (-y_1 - 1) e^{-y_1} \right]^\infty_0
  = 1
\end{align*}
for \(0 < y_2 < 1\). Otherwise, \(g(y_1, y_2)\) is zero so \(h_2(y_2) = 0\).
Then,
\begin{align*}
  h_1(y_1) h_2(y_2)
  = \begin{cases}
    e^{-y_1} |y_1| & (y_1 > 0, 0 < y_2 < 1) \\
    0 & (\text{elsewhere})
  \end{cases}
\end{align*}
As \(y_1 y_2 > 0\) and \(y_1 - y_1 y_2 > 0\) is equivalent to \(y_1 > 0\) and
\(0 < y_2 < 1\), \(g(y_1, y_2) = h_1(y_1) h_2(y_2)\) holds so \(Y_1\) and
\(Y_2\) are independent.

\section{Chapter 7 \#18}
We can write
\begin{align*}
  M_X(t)
  = \sum^\infty_{x = 1} e^{tx} pq^{x - 1}
  = \sum^\infty_{x = 0} e^{t(x + 1)} pq^x
  = p e^t \sum^\infty_{x = 0} (qe^t)^x
  = \frac{p e^t}{1 - q e^t}
\end{align*}
using the Taylor series expansion of geometric series. Now, we can write
\begin{align*}
  E(X)
  = \frac{d}{dt} M_X(t) \Bigr|_{t = 0}
  = \frac{pe^t}{(1 - q e^t)^2} \Bigr|_{t = 0}
  = \frac{p}{(1 - q)^2}
  = \frac{1}{p}
\end{align*}
So the mean of \(X\) is \(1 / p\). We can also write
\begin{align*}
  E(X^2)
  = \left( \frac{d}{dt} \right)^2 M_X(t) \Bigr|_{t = 0}
  = \frac{p e^t (1 + q e^t)}{(1 - q e^t)^3} \Bigr|_{t = 0}
  = \frac{1 + q}{p^2}
\end{align*}
Thus, the variance of \(X\) is \(E(X^2) - (E(X))^2 = q / p^2\).

\section{Chapter 7 \#23}
\subsection{Solution for (a)}
We can write
\begin{align*}
  P(U = u)
  = \sum^u_{x = 0} P(X = x) P(Y = u - x)
  = \sum^u_{x = 0} (q^x p) (q^{u - x} p)
  = \sum^u_{x = 0} q^u p^2
  = (u + 1) q^u p^2
\end{align*}
for \(u = 0, 1, 2, \dots\). (otherwise the probability is zero)

\subsection{Solution for (b)}
For \(x \le u\), we can write
\begin{align*}
  P(X = x, X + Y = u)
  &= P(X = x, Y = u - x)
  = P(X = x) P(Y = u - x) \\
  &= (q^x p) (q^{u - x} p)
  = q^u p^2
\end{align*}
Using the definition of conditional probability,
\begin{align*}
  P(X = x | X + Y = u)
  = \frac{P(X = x, X + Y = u)}{P(X + Y = u)}
  = \frac{q^u p^2}{(u + 1) q^u p^2}
  = \frac{1}{u + 1}
\end{align*}

\section{Supplemental Exercises \#1}
We can write
\begin{align*}
  P(D = 0)
  &= \sum^5_{x = 0} P(X_1 = x) P(X_2 = x)
  = 6 \cdot \frac{1}{6^2}
  = \frac{1}{6} \\
  P(D = 1)
  &= \sum^4_{x = 0} P(X_1 = x) P(X_2 = x + 1)
    + \sum^4_{x = 0} P(X_1 = x + 1) P(X_2 = x) \\
  &= 2 \cdot 5 \cdot \frac{1}{6^2}
  = \frac{5}{18} \\
  P(D = 2)
  &= \sum^3_{x = 0} P(X_1 = x) P(X_2 = x + 2)
    + \sum^3_{x = 0} P(X_1 = x + 2) P(X_2 = x) \\
  &= 2 \cdot 4 \cdot \frac{1}{6^2}
  = \frac{2}{9} \\
  P(D = 3)
  &= \sum^2_{x = 0} P(X_1 = x) P(X_2 = x + 3)
    + \sum^2_{x = 0} P(X_1 = x + 3) P(X_2 = x) \\
  &= 2 \cdot 3 \cdot \frac{1}{6^2}
  = \frac{1}{6} \\
  P(D = 4)
  &= \sum^1_{x = 0} P(X_1 = x) P(X_2 = x + 4)
    + \sum^1_{x = 0} P(X_1 = x + 4) P(X_2 = x) \\
  &= 2 \cdot 2 \cdot \frac{1}{6^2}
  = \frac{1}{9} \\
  P(D = 5)
  &= \sum^0_{x = 0} P(X_1 = x) P(X_2 = x + 5)
    + \sum^0_{x = 0} P(X_1 = x + 5) P(X_2 = x) \\
  &= 2 \cdot 1 \cdot \frac{1}{6^2}
  = \frac{1}{18}
\end{align*}

\section{Supplemental Exercises \#3 (a)}
Let \(\phi(x)\) be probability density function of standard normal
distribution. Since \(\mathrm{Cov}((X_1 + 1)^2, X_2) = 0\),
\begin{align*}
  E(Y)
  &= E((X_1 + 1)^2 + X_2)
  = E((X_1 + 1)^2) + E(X_2)
  = \int^\infty_{-\infty} (x + 1)^2 \phi(x) dx \\
  &= \int^\infty_{-\infty} x^2 \phi(x) dx + \int^\infty_{-\infty} 2x \phi(x) dx
    + \int^\infty_{-\infty} \phi(x) dx
  = E(X^2_1) + E(X_1) + 1 \\
  &= \mathrm{Var}(X_1) + (E(X_1))^2 + E(X_1) + 1
  = 2
\end{align*}

\end{document}
% vim: textwidth=79
